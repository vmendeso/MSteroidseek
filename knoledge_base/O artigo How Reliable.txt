O artigo "How Reliable are LLMs as Knowledge Bases? Re-thinking Factuality and Consistency" investiga a confiabilidade de modelos de linguagem (LLMs) como bases de conhecimento (KBs), desafiando a abordagem tradicional focada unicamente na retenção de conhecimento. Os autores argumentam que, para considerar um LLM confiável como KB, é preciso avaliar duas dimensões essenciais: factualidade (capacidade de fornecer respostas corretas tanto para informações vistas durante o treinamento quanto para as não vistas) e consistência (manutenção de respostas estáveis para perguntas sobre o mesmo conteúdo). Para isso, eles propõem novas métricas e introduzem dois conjuntos de dados: SeenQA, com fatos presumivelmente conhecidos pelos modelos, e UnseenQA, com perguntas sobre conhecimentos não acessíveis aos modelos antes de abril de 2024.

Os autores conduziram experimentos com 26 LLMs populares e observaram que modelos como o GPT-3.5-Turbo demonstram desempenho equilibrado em factualidade e consistência. Embora modelos maiores, como LLAMA3-70B, tenham uma taxa de acerto elevada em conhecimento visto, também apresentam maior taxa de erro e são mais propensos a fornecer respostas erradas de forma consistente. Um achado importante é que a factualidade não implica necessariamente confiabilidade, pois modelos podem ser consistentes mesmo quando estão errados. Além disso, os autores mostram que técnicas como ajuste fino melhoram o desempenho em conhecimento não visto, mas podem prejudicar a performance em conhecimento já conhecido.

Por fim, o estudo mostra que modelos grandes tendem a ser mais confiantes, inclusive ao fornecer informações incorretas, o que pode gerar riscos de desinformação. O uso de in-context learning (ICL) com exemplos incertos pode ajudar a reduzir respostas erradas em conhecimento desconhecido. Os autores concluem que o campo precisa de métricas mais robustas e uma abordagem mais crítica para garantir que os LLMs possam ser usados com segurança e confiabilidade como bases de conhecimento em aplicações do mundo real.
